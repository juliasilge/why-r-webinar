
# Understanding Word Embeddings

**Slides for my statistics seminar at BYU on 12 March 2020**

Modern natural language processing frameworks (including word2vec, GloVe, fastText, ULMFIT, and more) depend on word embeddings, a way of statistically modeling language where words or phrases are mapped to vectors of real numbers. In this talk, understand word embeddings by investigating how we can generate them using count-based statistics and dimensionality reduction, then learn how to make use of pre-trained embeddings based on enormous datasets. Finally, explore the ethical issues involved in using word embeddings and how they can amplify systemic and historical bias.

Check out the slides at [bit.ly/silge-byu](https://bit.ly/silge-byu)

Slides created with [remark.js](http://remarkjs.com/) and the R package [**xaringan**](https://github.com/yihui/xaringan)
